# mental-navigation-meg
Mental Navigation for MEG 

# TODO
## Jan 27 
0. Add a smaller window to redcuce field of view to ( 3 images ) replace with gradient 
0.1. Add a square at the center, this helps participants to be accurate.( low priority ; implement and see )
0.2. Record the file for each run; also for edf 
0.3 Try increase ILM distance

1. Select appropriate images Change images so category is cat/dog etc
 https://chatgpt.com/c/69789466-95d8-832f-be19-5c21e6b5e0cc ; wordfreq packages 
2. Add image presentation in for decoding at end of experiment [One image at a time ; centrally; duration 500ms; review papers;marius peelen (decoding meg)]
4. Cue at the start of each block, not in every trial . DONE. (FAST or slow text)
5. Add in one or two cycles while text is displayed  
1 loop
add more information in speed
(blank screen (for some sec) then speed cue) ( fixation cross and change color for movement ) 

7. Add in resting state data collection between runs [During break stop eye tracking or meg and collect data??] 

8. Free navigation at start [?? is this where participants are allowed to press a button to move left or right???]
  make a different script 
run only on first day to familiarize the environment
(duration : 4 minutes (max) ; add stop key )

# cedrus - look in to giuliano's code for ptb functions? (  )

9. Questionnaire, vividness validated question ( find validated questionnaires ; common psychological questions - chatgpt)

10. How to test meg ttls? is there a dummy mode? (A overview of meg code?)
 refer to photo diode code : gf5_present_letters  (search photo diode) 
use this for relevant events

# prioritize stimuli : 
share screenshots


1. Selection of Stimuli [ to be discussed]
    1. Icons
    2. Realistic images with color [ replace background with solid background]
    3. Look for datasets that use animals with solid bg 
    Animals : Dog Cat Pig Rat Cow Donkey

2. Replace the speed cues as : FAST or SLOW . DONE .
3. Add instructions on the speed cue  [ added placeholder]
4. Replace do with fixation corss and change color of the fixation cross when movement starts . DONE .
5. Save data file at each run ( behavior , eye and meg(manual) ) . DONE .
6. Make a seperate script for free navigation. Free navigation is used for training before the actual experiment. DONE .
8. Add contrast gradient to images such that only three images are visible at a time . DONE for visual ; if it is okay will implement to mental.
7. Find common psychological questions and validated questionnaires
11. Test data collection
6. At the start of each trial, show first two images [TBD ]


Next meeting :
1. Selection of Stimuli [ to be discussed]
9. Add a square at the center, this may participants to get an accurate feedback [to be discussed]
10. Add a input at start to select InterLandmarkDistance ( try different ILDs). ( we can try this from setup_exp_env and test in meetings)

Technical 
12. When to flip the diode??

Discuss about sessions
option 1 
Manual visual navigation each day before experiment
Session 1: Visual1 Visual2 Mental2  Mental1 Visual2 Visual1 Mental1 Mental2  Visual [Sequence learning run]
session 2: Mental [Mental Navigation test runs]

option 2
Session 1: Visual1 Visual2 Mental2  Mental1 Visual2 Visual1  [Mental nav training run ] Visual [Sequence learning runs]
session 2: Visual1 Visual2 Visual1 Visual2 Visual1 Visual 2 [Sequence learning run] Mental [Mental Navigation test run]
session 3: mental navigation test run

option 3


TODO 
Add correct/score ( 50% threshold); store targetPos; 70% of trials within an accuracy threshold (measured by how far participants are from the center of the frame),
Modeling different strategies
Prediction for strategies: 
Generate Ai generated icons 
Animal categories :

Animals : Dog  Cow Cat  Rooster  Fox Mouse
Animali: Cane,  Mucca, Gatto  Gallina, Volpe, Topo

colors : Dog — #FFB3B3 (soft red)

Cow — #FFD1A3 (soft orange)

Cat — #FFF3A6 (soft yellow)

Hen — #BFF2C1 (soft green)

Fox — #B3D9FF (soft blue)

Rat — #D6B3FF (soft violet)

train move -> hide target  -> reach 2nd image -> hide train

the goal of blink fix ation is to make partiicpant ready for the trial while not allowing them to do offline simulation
what if we show the image train during blink fixation but not the target. this will penalize them if they do offline simulation. becuase if they start early they   

# TODO
generate and change stimuli 

Explicit instruction ; no other instruction in speed cue
Strictly instruct Mentally simulate and not use movement 

set FEedback to 1s //

set ITI unif(2,3)

set regular blink (800ms on 50 ms off)x3

number of channels for sensors in meg?

Button hold instead of press 

#https://elifesciences.org/articles/48971

